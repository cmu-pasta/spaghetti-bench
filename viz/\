<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Why Controlled Concurrency Testing Matters - Spaghetti Bench</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
            background: #fff;
            min-height: 100vh;
            color: #333;
            line-height: 1.6;
        }

        .nav-bar {
            background: #fff;
            padding: 16px 24px;
            display: flex;
            align-items: center;
            justify-content: space-between;
            border-bottom: 1px solid #eee;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav-brand {
            color: #333;
            text-decoration: none;
            font-size: 18px;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .nav-links {
            display: flex;
            gap: 8px;
        }

        .nav-link {
            color: #666;
            text-decoration: none;
            font-size: 14px;
            padding: 8px 16px;
            border-radius: 6px;
            transition: all 0.2s;
        }

        .nav-link:hover {
            background: #f5f5f5;
            color: #333;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 60px 20px;
        }

        article {
            margin-bottom: 60px;
        }

        h1 {
            font-size: 36px;
            font-weight: 700;
            margin-bottom: 16px;
            color: #333;
        }

        .post-meta {
            color: #999;
            font-size: 14px;
            margin-bottom: 32px;
            padding-bottom: 32px;
            border-bottom: 1px solid #eee;
        }

        h2 {
            font-size: 28px;
            font-weight: 600;
            margin: 48px 0 20px;
            color: #333;
        }

        h3 {
            font-size: 22px;
            font-weight: 600;
            margin: 32px 0 16px;
            color: #444;
        }

        p {
            font-size: 17px;
            line-height: 1.8;
            color: #444;
            margin-bottom: 20px;
        }

        strong {
            color: #333;
            font-weight: 600;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            color: #d63384;
        }

        pre {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            margin: 24px 0;
            font-size: 14px;
            line-height: 1.5;
        }

        pre code {
            background: none;
            padding: 0;
            color: #333;
            font-size: inherit;
        }

        .highlight {
            background: #fff3cd;
            padding: 2px 4px;
            border-radius: 3px;
        }

        .callout {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 32px 0;
            border-radius: 4px;
        }

        .callout p {
            margin-bottom: 12px;
        }

        .callout p:last-child {
            margin-bottom: 0;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 32px 0;
        }

        .stat-card {
            background: #f8f9fa;
            padding: 24px;
            border-radius: 8px;
            text-align: center;
            border: 1px solid #e9ecef;
        }

        .stat-value {
            font-size: 36px;
            font-weight: 700;
            color: #667eea;
            display: block;
            margin-bottom: 8px;
        }

        .stat-label {
            font-size: 14px;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 32px 0;
            font-size: 15px;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }

        .comparison-table th {
            background: #f8f9fa;
            font-weight: 600;
            color: #333;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        .success {
            color: #28a745;
            font-weight: 600;
        }

        .failure {
            color: #dc3545;
            font-weight: 600;
        }

        .diff-add {
            background: #d4edda;
            color: #155724;
        }

        .diff-remove {
            background: #f8d7da;
            color: #721c24;
        }

        a {
            color: #667eea;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        footer {
            text-align: center;
            margin-top: 60px;
            padding-top: 32px;
            border-top: 1px solid #eee;
            color: #999;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <nav class="nav-bar">
        <a href="/" class="nav-brand">üçù Spaghetti Bench</a>
        <div class="nav-links">
            <a href="/leaderboard" class="nav-link">Leaderboard</a>
            <a href="/traces" class="nav-link">Examples</a>
            <a href="/blog" class="nav-link">Blog</a>
        </div>
    </nav>

    <div class="container">
        <article>
            <h1>Why Controlled Concurrency Testing Matters for AI Agents</h1>
            <div class="post-meta">February 8, 2026 ‚Ä¢ 10 min read</div>

            <p>
                Software engineering agents are becoming increasingly prevalent in software development. As these AI-powered
                tools take on more complex programming tasks, it's crucial to understand their performance across a variety
                of challenging domains. Benchmarks like <a href="https://www.swebench.com/" target="_blank">SWE-bench</a>
                have emerged as the standard for measuring coding agent performance on solving real-world software issues,
                evaluating agents on hundreds of GitHub issues from popular open-source projects.
            </p>

            <p>
                However, <strong>concurrency</strong>‚Äîa critical aspect of modern software engineering‚Äîis notably
                underrepresented in SWE-bench. Among hundreds of tasks, there is only
                <a href="https://github.com/astropy/astropy/pull/18692" target="_blank">one example</a> of a race condition.
                This gap is significant because concurrent programming is fundamental to performance-critical systems,
                from web servers and databases to distributed systems and mobile applications.
            </p>

            <p>
                The challenge with evaluating concurrency bug fixes goes beyond just dataset coverage. SWE-bench validates
                proposed patches by running specific tests: those that should fail on the buggy code must pass after the fix,
                while existing tests should continue passing. This approach can work for deterministic bugs, but
                <strong>breaks down for concurrency issues</strong>. A test for a concurrency bug may pass or fail depending
                on thread scheduling‚Äîmeaning a test might pass even when the underlying bug remains unfixed, simply because
                the problematic thread interleaving didn't occur during that particular test run.
            </p>

            <p>
                We evaluated leading AI coding agents on concurrency bug fixes and found an important insight:
                <strong>without proper testing tools, even the most capable models produce patches that appear correct
                but silently fail under specific thread interleavings</strong>. This post explores how controlled
                concurrency testing with <a href="https://github.com/cmu-pasta/fray" target="_blank">Fray</a> reveals
                these limitations and why it's essential for reliable concurrency bug verification.
            </p>

            <h2>Setup</h2>

            <p>
                We evaluated six state-of-the-art models using <a href="https://docs.openhands.dev/" target="_blank">OpenHands</a>,
                an agentic framework for software engineering tasks:
            </p>
            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li>Claude Opus 4.5</li>
                <li>Claude Sonnet 4.5</li>
                <li>OpenAI GPT-5.2</li>
                <li>Qwen 3 Coder 480B</li>
                <li>Google Gemini 3 Pro Preview</li>
                <li>Google Gemini 3 Flash Preview</li>
            </ul>

            <p>
                Each model was tested on 39 Java concurrency bugs from our dataset, which includes tasks that Fray was previously
                evaluated on. This includes 28 single-class programs from SCTBench, and 11 bugs from the open source Apache Kafka project. 
                All of these bugs have the property that running the tests thousands of times would not
                reliably trigger the bug‚Äîthe race conditions are too subtle and dependent on specific thread interleavings. However,
                Fray consistently finds these bugs within seconds.
            </p>

            <p>
                We ran each model in two configurations:
            </p>
            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li><strong>Without Fray:</strong> Agents have access to standard development tools (bash, file editor,
                test runner) but must verify their fixes using conventional approaches</li>
                <li><strong>With Fray:</strong> Agents can additionally invoke Fray to test their patches
                across different thread schedules</li>
            </ul>

            <p>
                In both configurations, we used Fray as the final arbiter to verify whether the agent's proposed solution
                actually fixed the bug. This means that even in the "without Fray" configuration, agents might produce
                patches they believe are correct (based on their own testing), but we verify correctness using
                Fray's interleaving exploration. If the agent failed to produce a patch within 20 minutes, we treated the result as a failed case.
            </p>

            <h2>How Agents Currently Behave</h2>

            <p>
                Without any additional tooling, AI agents typically verify concurrency fixes by running bash commands to run tests repeatedly:
            </p>

            <pre><code>for i in {1..100}; do
    echo "Run $i:";
    java MyProgram;
    echo "Exit code: $?";
done</code></pre>

            <p>
                The logic is intuitive: if the test passes 10, 20, or even 100 times in a row, the fix must be correct.
                Unfortunately, <span class="highlight">this approach gives a false sense of confidence</span>. The tests
                might pass simply because the problematic thread interleaving never occurred during those runs. This often results in the agent 
                finishing after its first proposed solution, as it assumes that the fix is correct from the test results.
            </p>

            <h3>Example: WorkStealQueue</h3>

            <p>
                Let's look at a concrete example from our benchmark: <a href="https://github.com/cmu-pasta/spaghetti-bench/blob/main/benchmarks/SCTBench/chess/WorkStealQueue.java" target="_blank">WorkStealQueue</a>.
                This is a lock-free work-stealing queue‚Äîa concurrent data structure where a single owner thread can
                <code>push</code> and <code>pop</code> items from one end (the tail), while multiple "stealer" threads
                grab work from the other end (the head). The test creates items, pushes them to the queue, and then
                has both the owner and stealers process them. Each item should be processed exactly once, verified by
                checking that <code>field == 1</code>.
            </p>

            <p>
                <strong>The Bug:</strong> The original code has a race condition in the <code>pop()</code> method.
                When the queue has exactly one element, the owner decrements the tail and checks
                <code>if (head &lt;= tail)</code>. If a stealer concurrently increments head, both threads might
                think they successfully got the item, or the owner might read a stale value from the <code>elems</code>
                array since it's not volatile. This leads to items being processed multiple times or not at all.
            </p>

            <p>
                <strong>Claude Sonnet 4.5</strong> attempted this task in both configurations. Without Fray, the agent
                produced a minimal fix:
            </p>

            <pre><code>// Changed comparison in pop() method
-if (readV(head) <= t) {
+if (readV(head) < t) {
     result[0] = elems[(int) (t & mask)];
     return true;
}</code></pre>

            <p>
                This single-character change (<code>&lt;=</code> to <code>&lt;</code>) tried to fix the boundary condition.
                However, <strong>this doesn't actually solve the problem</strong>. The issue isn't just the comparison‚Äîit's
                that the check-then-act sequence is not atomic. Even with <code>&lt;</code>, a stealer can still increment
                head between the check and the array access, and the owner still reads from a non-volatile array that may
                have stale values. The agent tested the fix and marked it as complete. However, when we ran final verification:
            </p>

            <pre><code>Iteration 2616: Error found at step 105
AssertionError at WorkStealQueue$ObjType.check</code></pre>

            <p>
                <strong>Fray found the bug</strong> after 2,616 iterations‚Äîdemonstrating how subtle these race conditions
                can be. The fix was insufficient. Without Fray to guide iteration, the agent stopped too early with an
                incomplete solution.
            </p>

            <p>
                When agents have access to Fray as a tool call, they can run it to verify fixes across thousands of different thread interleavings,
                and use it to iterate on solutions that would not have been caught by the repeated unit tests.
            </p>
            <p>
                We can contrast this with the <strong>same model on the same task with Fray access</strong>. Over the course
                of 5 refinements of its solution with Fray, the agent produced a <a href="/traces?model=bedrock_global.anthropic.claude-sonnet-4-5-20250929-v1_0%2Fwith_fray&category=sctbench#trace/bedrock_global.anthropic.claude-sonnet-4-5-20250929-v1_0/with_fray/rep_3/fix_bug/sctbench/WorkStealQueue.json">comprehensive fix</a> addressing multiple race conditions:
            </p>

            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li>Made <code>elems</code> array and <code>mask</code> volatile to ensure visibility across threads</li>
                <li>Captured <code>mask</code> locally to prevent inconsistent indexing during array resizing</li>
                <li>Completely rewrote <code>pop()</code> to properly handle the one-element case with locking and double-checking</li>
                <li>Added stability checks in <code>push()</code> to detect when the array is resized mid-operation</li>
                <li>Fixed the boundary condition from the first attempt (<code>&lt;=</code> to <code>&lt;</code>)</li>
            </ul>

            <p>
                On this patch, Fray verification passed 100,000 iterations with no errors. By providing a stronger verification 
                as a tool, the agent is able to propose more complex patches, rather than defaulting to the first and often laziest one.
            </p>

            <h3>Example: Apache Kafka ‚Äî Task Deduplication Bug</h3>

            <p>
                A real-world example from Apache Kafka demonstrates how agents can produce plausible-looking fixes that
                still fail under controlled-concurrency testing. The bug occurs in <code>DefaultStateUpdater</code>, which manages
                state restoration for Kafka Streams tasks across multiple threads. When calling <code>getTasks()</code>,
                tasks could transiently appear in multiple internal queues during state transitions, potentially causing
                duplicate tasks to be returned.
            </p>

            <p>
                <strong>GPT-5.2</strong> (without Fray) analyzed the error message showing the test expected 2 tasks but
                got 3, concluding that tasks were being duplicated in the output. The agent reasoned that tasks might
                transiently appear in multiple internal queues during state transitions. Based on this understanding,
                the agent attempted to fix the bug with a deduplication approach:
            </p>

            <pre><code>@Override
public Set<Task> getTasks() {
-   return executeWithQueuesLocked(() -> getStreamOfTasks()
-       .map(ReadOnlyTask::new)
-       .collect(Collectors.toSet()));
+   return executeWithQueuesLocked(
+       () -> getStreamOfTasks()
+           // Tasks may transiently appear in multiple internal queues during state transitions.
+           // Deduplicate the underlying tasks before wrapping them in ReadOnlyTask to avoid
+           // returning duplicates to callers.
+           .collect(Collectors.toSet())
+           .stream()
+           .map(ReadOnlyTask::new)
+           .collect(Collectors.toSet())
+   );
}</code></pre>

            <p>
                The agent's logic seemed sound: first collect the underlying tasks into a Set (which automatically
                deduplicates by object identity), then stream again to wrap each unique task in <code>ReadOnlyTask</code>.
                The agent added a detailed comment explaining this reasoning and believed the problem was solved.
            </p>

            <p>
                However, when Fray verified this patch:
            </p>

            <pre><code>Iteration 15: Error found at step 8179
AssertionFailedError: expected: <0> but was: <1></code></pre>

            <p>
                <strong>The fix didn't work.</strong> After 15 iterations exploring different thread interleavings, Fray
                found a scenario where the test still failed. The deduplication approach treated the symptom, not the cause.
            </p>

            <p>
                Compare this to the <a href="https://github.com/apache/kafka/pull/18607" target="_blank">actual fix</a>
                that Kafka developers merged:
            </p>

            <pre><code>private void maybeCompleteRestoration(final StreamTask task, ...) {
    ...
    changelogReader.unregister(changelogPartitions);
    addToRestoredTasks(task);
-   updatingTasks.remove(task.id());  // MOVED FROM HERE
    log.info("Stateful active task " + task.id() + " completed restoration");
}

private void addToRestoredTasks(final StreamTask task) {
    restoredActiveTasksLock.lock();
    try {
        restoredActiveTasks.add(task);
+       updatingTasks.remove(task.id());  // TO INSIDE THE LOCK
        log.debug("Active task " + task.id() + " was added to the restored tasks");
    } finally {
        restoredActiveTasksLock.unlock();
    }
}</code></pre>

            <p>
                The real issue was that <code>updatingTasks.remove()</code> was called <em>outside</em> the lock, creating
                a race condition. The correct fix moves this single line <em>inside</em> the lock to ensure atomic state
                transitions. This is a fundamentally different approach from the agent's deduplication strategy.
            </p>

            <div class="callout">
                <p><strong>Key Insight:</strong> The agent produced a reasonable-looking fix with clear documentation that
                addressed the symptoms (duplicate tasks in output), but missed the root cause (race condition in state management).
                Without systematic concurrency testing, there was no way to know the fix was incorrect. This is exactly
                the kind of false confidence that makes concurrency bugs so dangerous‚Äîthe fix seems right, passes basic
                testing, but fails under specific thread interleavings that only tools like Fray can reliably trigger.</p>
            </div>

            <h2>The Results: Fray's Impact</h2>

            <p>
                We evaluated multiple state-of-the-art models on 39 concurrency bug tasks, both with and without Fray.
                The results reveal different patterns for synthetic benchmarks versus real-world bugs.
            </p>

            <h3>SCTBench Performance (28 tasks)</h3>

            <p>
                On single-class synthetic benchmarks from SCTBench, Fray provides significant improvements across all models:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Without Fray (Pass@1)</th>
                        <th>With Fray (Pass@1)</th>
                        <th>Change</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Claude Opus 4.5</td>
                        <td>84.2%</td>
                        <td>92.9%</td>
                        <td class="success">+8.7%</td>
                    </tr>
                    <tr>
                        <td>Claude Sonnet 4.5</td>
                        <td>85.7%</td>
                        <td>92.9%</td>
                        <td class="success">+7.2%</td>
                    </tr>
                    <tr>
                        <td>GPT-5.2</td>
                        <td>88.1%</td>
                        <td>92.1%</td>
                        <td class="success">+4.0%</td>
                    </tr>
                    <tr>
                        <td>Qwen 3 Coder 480B</td>
                        <td>65.0%</td>
                        <td>66.4%</td>
                        <td class="success">+1.4%</td>
                    </tr>
                </tbody>
            </table>

            <p>
                The improvements are substantial, particularly for top-tier models. Fray helps catch false positives where
                agents produce patches that seem correct but fail under systematic testing. The WorkStealQueue example
                demonstrates this pattern clearly.
            </p>

            <h3>Real-World Kafka Performance (11 tasks)</h3>

            <p>
                On real-world Apache Kafka bugs, the picture is very different:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Without Fray (Pass@1)</th>
                        <th>With Fray (Pass@1)</th>
                        <th>Change</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Claude Opus 4.5</td>
                        <td>24.4%</td>
                        <td>31.1%</td>
                        <td class="success">+6.7%</td>
                    </tr>
                    <tr>
                        <td>Claude Sonnet 4.5</td>
                        <td>25.5%</td>
                        <td>31.1%</td>
                        <td class="success">+5.6%</td>
                    </tr>
                    <tr>
                        <td>GPT-5.2</td>
                        <td>22.2%</td>
                        <td>31.1%</td>
                        <td class="success">+8.9%</td>
                    </tr>
                    <tr>
                        <td>Qwen 3 Coder 480B</td>
                        <td>18.2%</td>
                        <td>15.6%</td>
                        <td class="failure">-2.6%</td>
                    </tr>
                </tbody>
            </table>

            <p>
                Performance on real-world issues is <strong>low with or without Fray</strong>, suggesting that Fray alone
                is insufficient for these more complex bugs. Real-world concurrency issues in large codebases like Kafka
                require deeper reasoning about system architecture, state management patterns, and the interaction between
                multiple components‚Äîchallenges that go beyond what improved verification tooling can address.
            </p>

            <p>
                This points to a critical gap: while Fray helps agents iterate and verify their fixes, agents still struggle
                with the initial diagnosis and reasoning required for complex real-world concurrency bugs.
                The Kafka task deduplication example illustrates this‚Äîthe agent correctly identified that tasks were being
                duplicated, but misdiagnosed the root cause, leading to a fix that addressed symptoms rather than the
                underlying race condition.
            </p>

            <h2>Takeaways</h2>

            <p>
                Our findings reveal both the value and limitations of verification tooling for AI coding agents:
            </p>

            <h3>1. Verification Tools Help, But Aren't Sufficient</h3>

            <p>
                Fray provides clear value for synthetic benchmarks (4-9% improvement), helping agents catch false positives
                and iterate toward correct solutions. However, on real-world bugs, even with Fray, success rates remain low
                (~25-31% for top models). This suggests that <strong>better verification alone cannot overcome fundamental
                reasoning gaps</strong> in how agents approach complex concurrency problems.
            </p>

            <h3>2. The Real Challenge: Interactive Debugging and Root Cause Analysis</h3>

            <p>
                The low real-world performance‚Äîwith or without Fray‚Äîpoints to a critical need for improved concurrency
                reasoning capabilities. Agents need better tools and strategies for:
            </p>
            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li><strong>Interactive debugging:</strong> Exploring thread interleavings and understanding how race conditions manifest</li>
                <li><strong>Root cause analysis:</strong> Distinguishing symptoms from causes (e.g., duplicate tasks vs. unsynchronized state updates)</li>
                <li><strong>Architectural understanding:</strong> Reasoning about state management patterns across multiple components</li>
                <li><strong>Hypothesis testing:</strong> Systematically exploring and eliminating potential causes</li>
            </ul>

            <p>
                The Kafka example demonstrates this gap: GPT-5.2 correctly identified the symptom (duplicate tasks) but
                failed to trace it back to the root cause (calling <code>remove()</code> outside the lock). Future work
                should focus on enhancing agents' ability to interactively explore and reason about concurrency issues,
                not just verify proposed fixes.
            </p>

            <h3>3. The False Positive Problem is Real</h3>

            <p>
                Standard benchmarking approaches that rely on test suites are insufficient for concurrency bugs. We found
                multiple cases where agents "succeeded" without Fray but actually produced incorrect patches. This suggests
                that <strong>concurrency bug benchmarks need specialized evaluation methodologies</strong>.
            </p>

            <h2>Try It Yourself</h2>

            <p>
                Spaghetti Bench is open source and available on <a href="https://github.com/cmu-pasta/spaghetti-bench" target="_blank">GitHub</a>.
                You can:
            </p>
            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li>Run the benchmark on your own models</li>
                <li>Explore the full traces of agent interactions</li>
                <li>Contribute new concurrency bug examples</li>
            </ul>

            <p>
                We also provide a <a href="/leaderboard">leaderboard</a> with detailed results for multiple models, and
                an <a href="/traces">examples page</a> where you can browse full conversation traces to see how different
                agents approach the same bugs.
            </p>

            <h2>Conclusion</h2>

            <p>
                Concurrency bugs represent a unique challenge for AI coding agents because standard verification
                approaches‚Äîrunning tests multiple times‚Äîprovide a false sense of security. Our evaluation shows that
                controlled concurrency testing tools like Fray are essential for:
            </p>
            <ul style="margin-left: 32px; margin-bottom: 20px;">
                <li><strong>Catching false positives</strong> that pass naive repeated testing</li>
                <li><strong>Guiding agents</strong> toward correct solutions through clear feedback</li>
                <li><strong>Providing reliable verification</strong> that fixes work across all thread interleavings</li>
            </ul>

            <p>
                As AI coding assistants become more capable, equipping them with specialized tools for challenging
                problem domains becomes increasingly important. Concurrency is just one example‚Äîwe expect similar
                patterns in other areas where standard testing is insufficient.
            </p>

            <p>
                For more details, explore our <a href="/leaderboard">leaderboard</a> or dive into specific
                <a href="/traces">example traces</a> to see how agents approach these challenging bugs.
            </p>
        </article>

        <footer>
            <p>Built by <a href="https://github.com/cmu-pasta" target="_blank">CMU PASTA Lab</a></p>
        </footer>
    </div>
</body>
</html>
